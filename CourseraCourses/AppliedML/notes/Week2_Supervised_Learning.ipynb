{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors & Supervised Learning\n",
    "\n",
    "- Supervised learning can be broken down into two categories:\n",
    "    - Classification - discrete class value\n",
    "    - Regression - continuous class value\n",
    "    \n",
    "    \n",
    "- KNN makes few assumptions about the structure of the date and gives potentially accurate and sometimes unstable predictions (sensitive to small changes in training data). \n",
    "\n",
    "\n",
    "- Linear model (fit using least-squares) makes strong assumptions about structure of data and gives stable but potentially inaccurate predictions\n",
    "\n",
    "\n",
    "- Model - A specific mathematical or computational description that expresses the relationship between a set of input variables and 1+ outcome variables that are being studied or predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "\n",
    "- Generalization ability refrs to an algorithm's ability to give accurate predictions for new, previously unseen data.\n",
    "\n",
    "\n",
    "- Assumptions:\n",
    "    - Future unseen data (test set) will have same properties as current training data.\n",
    "    - Thus, the models that are accurate on training set are expected to be accurate on test set.\n",
    "    - May not happen if trained model is tuned too specifically to training set.\n",
    "    \n",
    "\n",
    "- Models that are too complex for the amount of training data available are said to **overfit** and are not likely to generalize well to new examples.\n",
    "\n",
    "\n",
    "- Models that are too simple, that don't even do well on training data, are said to **underfit** and also are not likely to generalize well to new examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $R^2$ Regression Score\n",
    "\n",
    "- Measures how well a prediction model for regression fits given data\n",
    "\n",
    "\n",
    "- Score is between 0 and 1:\n",
    "    - Value of 0 corresponds to a constant model that predicts mean value of all training target values.\n",
    "    - Value of 1 corresponds to prefect prediction.\n",
    "    \n",
    "\n",
    "- Also known as \"coefficient of determination\"\n",
    "\n",
    "\n",
    "- KNN is a good baseline to compare against more complicated models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighbors Classifier & KNeighbors Regressor: Important Parameters\n",
    "\n",
    "- Model Complexity: \n",
    "    - n-neighbors: # of nearest neighbors (k) to consider\n",
    "        - Default: k = 5\n",
    "\n",
    "\n",
    "- Model Fitting:\n",
    "    - Metric: Distance function between data points\n",
    "        - Default: Minkowski distance with power parameter p = 2 (Euclidean distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares\n",
    "\n",
    "- A **linear model** is a sum of weighted variables that predicts a target output value given an input data instance. Example: Predicting housing prices.\n",
    "    - Housing features: taxes per year ($X_{Tax}$), age in years ($X_{Age}$)\n",
    "        - $\\hat{Y}$ = 212,000 + 109 $X_{Tax}$ - 2,000 $X_{Age}$\n",
    "    - A house with feature values ($X_{Tax}$, $X_{Age}$) of (10,000, 75) would have a predicted sale price of:\n",
    "        - $\\hat{Y}_{price}$ = 212,000 + 109(10,000) - 2,000(75) = 1,152,000 \n",
    "        \n",
    "\n",
    "- Input instance - feature vector: x = ($x_0$, $x_1$, ... , $x_n$)\n",
    "  \n",
    "  Predicted output: $\\hat{y}$ = $\\hat{w}_0 x_0 + \\hat{w}_1 x_1 + ... + \\hat{w}_n x_n + b$\n",
    "  \n",
    "  Parameters to estimate: $$\\hat{w} = (\\hat{w}_0, ..., \\hat{w}_n ) : feature \\ \\ weights/model \\ \\ coefficients $$\n",
    "  $$\\hat{b} = constant \\ \\ bias \\ \\ term/intercept $$\n",
    "  \n",
    "\n",
    "- $\\hat{y} = \\hat{w}_0 x_0 + \\hat{b}$ <br> $\\hat{w}_0$ (slope) <br> $\\hat{b}$ (y-int) \n",
    "\n",
    "\n",
    "- Least squares (or Ordinary Least Squares) finds *w* and *b* that minimize the mean squared error of the model: the sum of squared differences between predicted target and actualy target values.\n",
    "\n",
    "\n",
    "- No parameters to control model complexity\n",
    "\n",
    "\n",
    "- Parameters estimated from training data\n",
    "\n",
    "\n",
    "- Many different way to estimate *w* and *b*:\n",
    "    - Different methods correspond to different \"fit\" criteria and goals and ways to control model complexity\n",
    "    \n",
    "\n",
    "- The learning algorithm finds the parameters that optimize an objective function, typically to minimize some kind of loss function of the predicted target values vs. the actual target values\n",
    "\n",
    "\n",
    "- For LMs, model complexity is based on the nature of the weights, w, on the input features. Simpler LMs have a weight vector, w, that is closer to 0, i.e. where more features are either not used at all (that have zero weight) or have less influence on the outcome, a very small weight.\n",
    "\n",
    "$$ RSS(w,b) = \\sum \\limits_{i=1} ^{N} (y_i - (w \\cdot x_i + b)) ^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "- Ridge regression learns parameters w and b using the same OLS criterion buf adds a penalty for large variations in w parameters.\n",
    "\n",
    "$$ RSS_{Ridge} (w,b) = \\sum \\limits_{i=1} ^{N} (y_i - (w \\cdot x_i + b)) ^{2} $$\n",
    "\n",
    "- Once parameters are learned, the ridge regression prediction formula is the **same** as OLS.\n",
    "\n",
    "\n",
    "- The addition of a parameter penalty is called *regularization*. Regularization **prevents overfitting** by restricting the model, typically to reduce its complexity.\n",
    "\n",
    "\n",
    "- Ridge regression use *L2 regularization*: minimizes the sum of squares of w entries.\n",
    "\n",
    "\n",
    "- Influence of the regularization term is controlled by the $\\alpha \\$ parameter.\n",
    "\n",
    "\n",
    "- Higher $\\alpha \\$ means more regularization and simpler models.\n",
    "\n",
    "\n",
    "### Need fore Feature Normalization\n",
    "\n",
    "- Important for some ML methods that all features are on same scale (e.g. faster convergence in learning, more uniform of \"fair\" influence for all weights)\n",
    "    - E.g. Regularized regression, KNN, SVM, NNs, ...\n",
    "    \n",
    "\n",
    "- **Can also depend on the data.** For now, we do MinMax scaling on features.\n",
    "    - For each feature $x_i$: Compute the min value $x_i^{MIN}$ and max value $x_i ^{MAX}$ achieved across all instances in training set.\n",
    "    - For each feature: Transform a given feature $x_i$ value to a scaled version $x_i^{'}$ using the formula: $$ x_i ^{'} = \\frac{(x_i - x_i ^{MIN})}{(x_i ^{MAX} - x_i ^{MAX})} $$\n",
    "    \n",
    "\n",
    "- **Feature Normalization: Test set must use identical scaling to training set.**\n",
    "    - Fit scaler using training set, then apply same scaler to transform test set.\n",
    "    - Do not scale training and test sets using different scalers: **this could lead to random skew in the data, which will invalidate results.**\n",
    "    - Do not fit scaler using any part of the test data: **referencing the test data can lead to a form of data leakage.**\n",
    "    \n",
    "\n",
    "- Downside to normalization is that resulting model and transformed features may be harder to interpret.\n",
    "\n",
    "\n",
    "- **In general, regularization works well when you have relatively small amounts of training data compared to the number of features in your model.**\n",
    "\n",
    "\n",
    "- Regularization becomes less important as the amount of training data you have increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "- Another form of regularized linear regression is **Lasso regression**. It uses an *L1 regularization* penalty for training (instead of Ridge's L2 penalty).\n",
    "\n",
    "\n",
    "- L1 penalty: Minimum sum of *absolute values* of coefficients.\n",
    "$$ RSS_{Lasso} (w,b) = \\sum \\limits_{i=1}^{N} (y_i - (w \\cdot x_i + b))^{2} + \\alpha \\ \\sum \\limits_{j=1} ^{N} |w_j| $$\n",
    "\n",
    "\n",
    "- This has the effect of setting parameter weights in w to *zero* for the least influential variables. This is called a *sparse* solution: a kind of feature selection.\n",
    "\n",
    "\n",
    "- Parameter $\\alpha$ controls the amount of L1 regularization (default = 1.0).\n",
    "\n",
    "\n",
    "- Prediction formula is same as OLS.\n",
    "\n",
    "\n",
    "- **When to use Ridge vs. Lasso regression**:\n",
    "    - Many variables with small/medium sized effects: use Ridge\n",
    "    - Only a few variables with medium/large effect: use Lasso\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features with Linear Regression\n",
    "\n",
    "- x = ($x_0$, $x_1$) &emsp;====> $x^{'}$ = ($x_0, x_1, x_0 ^{2}, x_0x_1, x_1 ^{2})$ <br> $\\hat{y}$ = $\\hat{w_0}x_0$ + $\\hat{w_1}x_1$ + $\\hat{w}_{00}x_0 ^{2}$ + $\\hat{w}_{01} x_0 x_1$ + $\\hat{w}_{11} x_1 ^{2} + b $ \n",
    "\n",
    "\n",
    "- Generate new features consisting of all polynomial combinations of the original two features $(x_0, x_1)$\n",
    "\n",
    "\n",
    "- The degree of the polynomial specifies how many variables participate at a time in each new feature (above example: degree 2).\n",
    "\n",
    "\n",
    "- This is still a weighted linear combination of features, so it's still a linear model and can use least-squares estimation method for w and b.\n",
    "\n",
    "\n",
    "- **Why do this kind of transformation?**\n",
    "    - Called a *polynomial feature transformation*\n",
    "    - Can use to transform a problem into a higher dimensional regression space\n",
    "    - In effect, adding these extra polynomial features allows us much richer set of complex functions that we can use to fit the data\n",
    "    - Can think of this intuitively as allowing polynomials to be fit to the training data instead of simply a straight line, but using the same least-squares criterion that minimizes mean squared error\n",
    "    - To capture interactions between original features by adding them as features to the linear model\n",
    "    \n",
    "\n",
    "- Effective with classification.\n",
    "\n",
    "\n",
    "- More generally, we can apply other non-linear transformations to create new features (technically, these are called non-linear basis functions).\n",
    "\n",
    "\n",
    "- **Beware of polynomial feature expansion with higher degree, as this can lead to complex models that overfit**.\n",
    "    - Thus, polynomial feature expansion is often combined with a regularized method like Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "- Target is binary instead of continuous.\n",
    "\n",
    "\n",
    "- Uses the logistic function, which compresses the output of the linear function so that it's limited to a range between 0 and 1, interpreted as the *probability* the input object belongs to the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
