{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors & Supervised Learning\n",
    "\n",
    "- Supervised learning can be broken down into two categories:\n",
    "    - Classification - discrete class value\n",
    "    - Regression - continuous class value\n",
    "    \n",
    "    \n",
    "- KNN makes few assumptions about the structure of the date and gives potentially accurate and sometimes unstable predictions (sensitive to small changes in training data). \n",
    "\n",
    "\n",
    "- Linear model (fit using least-squares) makes strong assumptions about structure of data and gives stable but potentially inaccurate predictions\n",
    "\n",
    "\n",
    "- Model - A specific mathematical or computational description that expresses the relationship between a set of input variables and 1+ outcome variables that are being studied or predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "\n",
    "- Generalization ability refrs to an algorithm's ability to give accurate predictions for new, previously unseen data.\n",
    "\n",
    "\n",
    "- Assumptions:\n",
    "    - Future unseen data (test set) will have same properties as current training data.\n",
    "    - Thus, the models that are accurate on training set are expected to be accurate on test set.\n",
    "    - May not happen if trained model is tuned too specifically to training set.\n",
    "    \n",
    "\n",
    "- Models that are too complex for the amount of training data available are said to **overfit** and are not likely to generalize well to new examples.\n",
    "\n",
    "\n",
    "- Models that are too simple, that don't even do well on training data, are said to **underfit** and also are not likely to generalize well to new examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $R^2$ Regression Score\n",
    "\n",
    "- Measures how well a prediction model for regression fits given data\n",
    "\n",
    "\n",
    "- Score is between 0 and 1:\n",
    "    - Value of 0 corresponds to a constant model that predicts mean value of all training target values.\n",
    "    - Value of 1 corresponds to prefect prediction.\n",
    "    \n",
    "\n",
    "- Also known as \"coefficient of determination\"\n",
    "\n",
    "\n",
    "- KNN is a good baseline to compare against more complicated models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighbors Classifier & KNeighbors Regressor: Important Parameters\n",
    "\n",
    "- Model Complexity: \n",
    "    - n-neighbors: # of nearest neighbors (k) to consider\n",
    "        - Default: k = 5\n",
    "\n",
    "\n",
    "- Model Fitting:\n",
    "    - Metric: Distance function between data points\n",
    "        - Default: Minkowski distance with power parameter p = 2 (Euclidean distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares\n",
    "\n",
    "- A **linear model** is a sum of weighted variables that predicts a target output value given an input data instance. Example: Predicting housing prices.\n",
    "    - Housing features: taxes per year ($X_{Tax}$), age in years ($X_{Age}$)\n",
    "        - $\\hat{Y}$ = 212,000 + 109 $X_{Tax}$ - 2,000 $X_{Age}$\n",
    "    - A house with feature values ($X_{Tax}$, $X_{Age}$) of (10,000, 75) would have a predicted sale price of:\n",
    "        - $\\hat{Y}_{price}$ = 212,000 + 109(10,000) - 2,000(75) = 1,152,000 \n",
    "        \n",
    "\n",
    "- Input instance - feature vector: x = ($x_0$, $x_1$, ... , $x_n$)\n",
    "  \n",
    "  Predicted output: $\\hat{y}$ = $\\hat{w}_0 x_0 + \\hat{w}_1 x_1 + ... + \\hat{w}_n x_n + b$\n",
    "  \n",
    "  Parameters to estimate: $$\\hat{w} = (\\hat{w}_0, ..., \\hat{w}_n ) : feature \\ \\ weights/model \\ \\ coefficients $$\n",
    "  $$\\hat{b} = constant \\ \\ bias \\ \\ term/intercept $$\n",
    "  \n",
    "\n",
    "- $\\hat{y} = \\hat{w}_0 x_0 + \\hat{b}$ <br> $\\hat{w}_0$ (slope) <br> $\\hat{b}$ (y-int) \n",
    "\n",
    "\n",
    "- Least squares (or Ordinary Least Squares) finds *w* and *b* that minimize the mean squared error of the model: the sum of squared differences between predicted target and actualy target values.\n",
    "\n",
    "\n",
    "- No parameters to control model complexity\n",
    "\n",
    "\n",
    "- Parameters estimated from training data\n",
    "\n",
    "\n",
    "- Many different way to estimate *w* and *b*:\n",
    "    - Different methods correspond to different \"fit\" criteria and goals and ways to control model complexity\n",
    "    \n",
    "\n",
    "- The learning algorithm finds the parameters that optimize an objective function, typically to minimize some kind of loss function of the predicted target values vs. the actual target values\n",
    "\n",
    "\n",
    "- For LMs, model complexity is based on the nature of the weights, w, on the input features. Simpler LMs have a weight vector, w, that is closer to 0, i.e. where more features are either not used at all (that have zero weight) or have less influence on the outcome, a very small weight.\n",
    "\n",
    "$$ RSS(w,b) = \\sum \\limits_{i=1} ^{N} (y_i - (w \\cdot x_i + b)) ^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "- Ridge regression learns parameters w and b using the same OLS criterion buf adds a penalty for large variations in w parameters.\n",
    "\n",
    "$$ RSS_{Ridge} (w,b) = \\sum \\limits_{i=1} ^{N} (y_i - (w \\cdot x_i + b)) ^{2} $$\n",
    "\n",
    "- Once parameters are learned, the ridge regression prediction formula is the **same** as OLS.\n",
    "\n",
    "\n",
    "- The addition of a parameter penalty is called *regularization*. Regularization **prevents overfitting** by restricting the model, typically to reduce its complexity.\n",
    "\n",
    "\n",
    "- Ridge regression use *L2 regularization*: minimizes the sum of squares of w entries.\n",
    "\n",
    "\n",
    "- Influence of the regularization term is controlled by the $\\alpha \\$ parameter.\n",
    "\n",
    "\n",
    "- Higher $\\alpha \\$ means more regularization and simpler models.\n",
    "\n",
    "\n",
    "### Need fore Feature Normalization\n",
    "\n",
    "- Important for some ML methods that all features are on same scale (e.g. faster convergence in learning, more uniform of \"fair\" influence for all weights)\n",
    "    - E.g. Regularized regression, KNN, SVM, NNs, ...\n",
    "    \n",
    "\n",
    "- **Can also depend on the data.** For now, we do MinMax scaling on features.\n",
    "    - For each feature $x_i$: Compute the min value $x_i^{MIN}$ and max value $x_i ^{MAX}$ achieved across all instances in training set.\n",
    "    - For each feature: Transform a given feature $x_i$ value to a scaled version $x_i^{'}$ using the formula: $$ x_i ^{'} = \\frac{(x_i - x_i ^{MIN})}{(x_i ^{MAX} - x_i ^{MAX})} $$\n",
    "    \n",
    "\n",
    "- **Feature Normalization: Test set must use identical scaling to training set.**\n",
    "    - Fit scaler using training set, then apply same scaler to transform test set.\n",
    "    - Do not scale training and test sets using different scalers: **this could lead to random skew in the data, which will invalidate results.**\n",
    "    - Do not fit scaler using any part of the test data: **referencing the test data can lead to a form of data leakage.**\n",
    "    \n",
    "\n",
    "- Downside to normalization is that resulting model and transformed features may be harder to interpret.\n",
    "\n",
    "\n",
    "- In general, regularization works well when you have relatively small amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
